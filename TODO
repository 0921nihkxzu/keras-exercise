- implement following optimizers:
	- mini-batch - Done
	- batch gradient
	- momentum
	- RMSprop
	- Adam
	- learning rate decay